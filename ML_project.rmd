---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

#### Load packages


```{r, message=F, warning=F}
library(ggplot2)
```
```{r, message=F, warning=F}
library(dplyr)
```
```{r, message=F, warning=F}
library(statsr)
```
```{r, message=F, warning=F}
library(gbm)
```
```{r, message=F, warning=F}
library(caret)
```
```{r, message=F, warning=F}
library(AppliedPredictiveModeling)
```
```{r, message=F, warning=F}
library(elasticnet)
```
```{r, message=F, warning=F}
library(elasticnet)
```
```{r, message=F, warning=F}
library(lubridate)
```
```{r, message=F, warning=F}
library(e1071)
```





* * *

## Data

Let us quote Coursera's information on the basic data, refered here as $df$.*Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har*

Next we read the basic data $df$ and validation data $valid$. 
Then we remove all columns having missing values or irrelevant identification information. 
We do that such that $valid$ has the same columns as $df$, except for the *classe* column (dependent variable) containing information on the manner in which they did the exercise. 

```{r}
df <- read.csv("./pml-training.csv", header=TRUE)
valid <- read.csv("./pml-testing.csv", header=TRUE)
valid <- valid %>% select(-c(1,7),-"problem_id")
valid <- valid[, colSums(is.na(valid)) == 0]
df <- df %>% select("classe",colnames(valid))
##df <- df %>% na_if("") %>% na.omit
##df <- df[, colSums(is.na(df)) == 0]
```

Check the dimensions of $df$ and $valid$, and list predictor variables. 


```{r}
dim(df)
dim(valid)
colnames(df)
```


As the final step of data manipulation, we split $df$ into the training ($training$) and test sets ($testing$). Note that this step is necessary for testing the model,
since $valid$ has only 20 members without *classe* values.

```{r}
set.seed(100) 
t_c <- createDataPartition(df$classe, p = 0.7, list = FALSE)
training <- df[t_c, ]
testing <- df[-t_c, ]
testing$classe <- as.factor(testing$classe)
testing$classe <- as.factor(testing$classe)
```

* * *


## Model with cross-validation and its accuracy


Since we have 57 predictor variables and the sample size of $training$ is 13737, 
we should be able to build a sharp enough random forest (decision trees) model predicting *classe* values. 
We will test the model, and if the (out of sample) accuracy for the test set is less than 0.9, then we will change the model type. 
An alternative option is the gradient boosted (decision trees) method.

Let us create the random forest model with 20 trees.
We keep the number of trees moderate to limit the performance time.
If necessary, we may increase (or decrease) it later.
To reduce overfitting, we use the k-fold cross-validation with $k=4$. 

```{r}
cv <- trainControl(method='cv', number = 4)
mod <- train(classe ~ ., data=training, trControl=cv, method='rf', ntree=20)
```

Let us calculate the accuracy for the test set.

```{r}
pred <- predict(mod, newdata=testing)
cm <- confusionMatrix(pred, testing$classe)
cm$overall[1]

```

The accuracy is  0.9992, which is very promising. 
It also worth noting that the model is quite light, which means that the performance time is around a minute. 
It is hard to find a good reason to do essential changes to the model. 
However, if one would like to change the model type, a similar manner with minor modifications would work.


## Prediction

We finish this note by calculating *classe* values for the validation data $valid$.

```{r}
predict(mod, newdata=valid)
```





